Got it! Here‚Äôs the entire README written fully in Markdown (including installation and usage instructions formatted as code blocks):

# ü§ñ Context-Aware Chatbot

A context-aware chatbot built using **LangChain**, **Ollama**, and **SQLite**. This project leverages local LLMs via Ollama and uses SQLite for memory and context management.

---

## üõ†Ô∏è Features

- Local LLM-powered conversations using Ollama  
- Context management with SQLite  
- Built using LangChain for modularity and flexibility

---

## üì¶ Installation

### 1. Install dependencies

```bash
pip install langchain langchain-ollama sqlalchemy langchain-community
```
### 2. Install Ollama

Download and install Ollama from the official website:
https://ollama.com/

### 3. Pull a model

You can pull any supported model. For example I used minstral :

ollama pull mistral:7b-instruct-q4_K_M

You can change the model used in like 15/16 in both files depending on preference


### Usage

Once everything is set up, run the chatbot script:

python chatbot_async.py

The chatbot will launch and begin interacting using the selected local LLM with context-aware memory managed by SQLite.

You can use the chatbot_stream file as well. This file is faster abd gives a more realtime feel as response is seen while answer is being generated by the bot beause the response is being  streamed 


### Powered By
	‚Ä¢	LangChain
	‚Ä¢	Ollama
	‚Ä¢	SQLite


